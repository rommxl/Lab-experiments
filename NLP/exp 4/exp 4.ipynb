{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name** : Bodhisatya Ghosh \\\n",
    "**Class** : CSE DS \\\n",
    "**UID** : 2021700026 \\\n",
    "**Subject** : NLP \\\n",
    "**Experiment number** : 4 \\\n",
    "\\\n",
    "**Aim**:\n",
    "Perform document classification using NB(no use of library of NB classifier) \\\n",
    "Copy abstract of papers and create labelled dataset. \\\n",
    "Create word count vectors. Each row represents a document, and each column represents a word. \\\n",
    "Read text from set of test documents and classify the unlabelled documents. \\\n",
    "Generate confusion matrix, and calculate accuracy, precision, recall, F1 score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes algorithm is a supervised machine learning algorithm based on the Bayes’ theorem. It is a probabilistic classifier that is often used in NLP tasks like sentiment analysis (identifying a text corpus’ emotional or sentimental tone or opinion).\n",
    "\n",
    "The Bayes’ theorem is used to determine the probability of a hypothesis when prior knowledge is available. It depends on conditional probabilities. The formula is given below :\n",
    "\n",
    "Naive Bayes Classifier: \\\n",
    "P(A|B) is posterior probability i.e. the probability of a hypothesis A given the event B occurs. \\\n",
    "P(B|A) is likelihood probability i.e. the probability of the evidence given that hypothesis A is true. \\\n",
    "P(A) is prior probability i.e. the probability of the hypothesis before observing the evidence \\\n",
    "P(B) is marginal probability i.e. the probability of the evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import math\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data (Fake news dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data.csv\", usecols=['text','label'])\n",
    "df = data.copy().iloc[0:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing non-alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanText(string):\n",
    "    result = re.sub(r'[^A-Za-z]',' ',string)    #remove non-alphanumeric characters \n",
    "    result = re.sub(r'\\s+',' ',result)\n",
    "    result = result.lower()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda sent: cleanText(sent)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    st = \"\"\n",
    "    for w in w_tokenizer.tokenize(text):\n",
    "        st = st + lemmatizer.lemmatize(w) + \" \"\n",
    "    return st\n",
    "df['text'] = df.text.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data 80:20 train:test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text'].values\n",
    "label = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text, train_label, test_label =  train_test_split(text, label, random_state=100, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(max_features = 3000)\n",
    "X = vec.fit_transform(train_text)\n",
    "vocab = vec.get_feature_names_out()\n",
    "X = X.toarray()\n",
    "word_counts = {}\n",
    "for l in range(2):\n",
    "    word_counts[l] = defaultdict(lambda: 0)\n",
    "for i in range(X.shape[0]):\n",
    "    l = train_label[i]\n",
    "    for j in range(len(vocab)):\n",
    "        word_counts[l][vocab[j]] += X[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_smoothing(n_label_items, vocab, word_counts, word, text_label):\n",
    "    a = word_counts[text_label][word] + 1\n",
    "    b = n_label_items[text_label] + len(vocab)\n",
    "    return math.log(a/b)\n",
    "\n",
    "def group_by_label(x, y, labels):\n",
    "    data = {}\n",
    "    for l in labels:\n",
    "        data[l] = x[np.where(y == l)]\n",
    "    return data\n",
    "\n",
    "def fit(x, y, labels):\n",
    "    n_label_items = {}\n",
    "    log_label_priors = {}\n",
    "    n = len(x)\n",
    "    grouped_data = group_by_label(x, y, labels)\n",
    "    for l, data in grouped_data.items():\n",
    "        n_label_items[l] = len(data)\n",
    "        log_label_priors[l] = math.log(n_label_items[l] / n)\n",
    "    return n_label_items, log_label_priors\n",
    "\n",
    "def predict(n_label_items, vocab, word_counts, log_label_priors, labels, x):\n",
    "    result = []\n",
    "    for text in x:\n",
    "        label_scores = {l: log_label_priors[l] for l in labels}\n",
    "        words = set(w_tokenizer.tokenize(text))\n",
    "        for word in words:\n",
    "            if word not in vocab: continue\n",
    "            for l in labels:\n",
    "                log_w_given_l = laplace_smoothing(n_label_items, vocab, word_counts, word, l)\n",
    "                label_scores[l] += log_w_given_l\n",
    "        result.append(max(label_scores, key=label_scores.get))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction on test set :  0.7278590748318191\n",
      "Precision of prediction on test set :  0.9745921369350093\n",
      "Recall of prediction on test set :  0.48762210624916363\n",
      "F1 score of prediction on test set :  0.6500178380306814\n"
     ]
    }
   ],
   "source": [
    "labels = [0,1]\n",
    "n_label_items, log_label_priors = fit(train_text,train_label,labels)\n",
    "pred = predict(n_label_items, vocab, word_counts, log_label_priors, labels, test_text)\n",
    "print(\"Accuracy of prediction on test set : \", accuracy_score(test_label,pred))\n",
    "print(\"Precision of prediction on test set : \", precision_score(test_label,pred))\n",
    "print(\"Recall of prediction on test set : \", recall_score(test_label,pred))\n",
    "print(\"F1 score of prediction on test set : \", f1_score(test_label,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix of prediction on test set : \n",
      " [[6851   95]\n",
      " [3829 3644]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix of prediction on test set : \\n\", confusion_matrix(test_label,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curiosity questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the relation between accuracy and precision?\n",
    "Precision:\n",
    "\n",
    "Definition: Precision is the ratio of correctly predicted positive observations to the total predicted positives. It focuses on the accuracy of the positive predictions. \\\n",
    "Interpretation: High precision means that the model has fewer false positives. \n",
    "\n",
    "Accuracy:\n",
    "\n",
    "Definition: Accuracy is the ratio of correctly predicted observations (both true positives and true negatives) to the total observations.\\\n",
    "Interpretation: Accuracy provides an overall measure of the model's correctness, but it can be misleading if there is an imbalanced class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Give example where precision is significant compared to accuracy?\n",
    "\n",
    "Precision focuses more on the ability to predict a single class, rather than the overall ability to classify. Precision is more important than accuracy in situations where wrong predictions have heavy consequences. For example: Medical predicition of a rare disease should have high precision as predicting the wrong class may have dire consequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Give example where accuracy is significant compared to precision?\n",
    "\n",
    "Accuracy is more important than precision in situations where wrong predictions do not have serious implications. For example: Spam email classification should have a high accuracy the consequences of both false positives and false negatives are important, but neither is inherently more critical than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
