{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name** : Bodhisatya Ghosh \\\n",
    "**Class** : CSE DS \\\n",
    "**UID** : 2021700026 \\\n",
    "**Subject** : NLP \\\n",
    "**Experiment number** : 3 \\\n",
    "\\\n",
    "**Aim**:\n",
    "1. Calculate bigrams from a given corpus , display bigram probability table and calculate probability of a sentence.\n",
    "2. To apply add-one smoothing on sparse bigram table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "allText = pd.read_csv('../exp 2/reviews.csv',usecols=['review'])\n",
    "corpus = allText['review'][0] + ' ' +allText['review'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Perform preprocessing. (Covert data to lowercase, and remove punctuation marks and stop words to remove noise. Use the eos tag to mark the beginning and end of the sentence.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove html tags\n",
    "corpus = corpus.replace('<br />',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words\n",
    "def remove_stopwords_function(corpus): \n",
    "    remove_stopwords = []\n",
    "    for word in corpus.split():\n",
    "        if(word not in stopwords.words()):\n",
    "            remove_stopwords.append(word)\n",
    "    return \" \".join(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One reviewers mentioned watching 1 Oz episode hooked. They right, happened me. The struck Oz brutality unflinching scenes violence, set word GO. Trust me, show faint hearted timid. This show pulls punches drugs, sex violence. Its hardcore, classic word. It called OZ nickname Oswald Maximum Security State Penitentary. It focuses Emerald City, experimental section prison cells glass fronts inwards, privacy high agenda. Em City home many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish more....so scuffles, death stares, dodgy dealings shady agreements away. I appeal show due fact shows dare. Forget pretty pictures painted mainstream audiences, forget charm, forget romance...OZ mess around. The episode I struck nasty surreal, I I ready it, I watched more, I developed taste Oz, accustomed high levels graphic violence. Not violence, injustice (crooked guards who\\'ll sold nickel, inmates who\\'ll kill order away it, mannered, middle class inmates turned prison bitches due lack street skills prison experience) Watching Oz, comfortable uncomfortable viewing....thats touch darker side. A wonderful production. The filming technique unassuming- old-time-BBC fashion comforting, discomforting, realism entire piece. The actors extremely chosen- Michael Sheen \"has polari\" voices pat too! You seamless editing guided references Williams\\' diary entries, worth watching terrificly written performed piece. A masterful production great master\\'s comedy life. The realism home things: fantasy guard which, traditional \\'dream\\' techniques remains solid disappears. It plays knowledge senses, particularly scenes concerning Orton Halliwell sets (particularly flat Halliwell\\'s murals decorating surface) terribly done.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = remove_stopwords_function(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SOSTOKEN one reviewers mentioned watching 1 oz episode hooked EOSTOKEN SOSTOKEN they right happened me EOSTOKEN SOSTOKEN the struck oz brutality unflinching scenes violence set word go EOSTOKEN SOSTOKEN trust me show faint hearted timid EOSTOKEN SOSTOKEN this show pulls punches drugs sex violence EOSTOKEN SOSTOKEN its hardcore classic word EOSTOKEN SOSTOKEN it called oz nickname oswald maximum security state penitentary EOSTOKEN SOSTOKEN it focuses emerald city experimental section prison cells glass fronts inwards privacy high agenda EOSTOKEN SOSTOKEN em city home many aryans muslims gangstas latinos christians italians irish more so scuffles death stares dodgy dealings shady agreements away EOSTOKEN SOSTOKEN i appeal show due fact shows dare EOSTOKEN SOSTOKEN forget pretty pictures painted mainstream audiences forget charm forget romance oz mess around EOSTOKEN SOSTOKEN the episode i struck nasty surreal i i ready it i watched more i developed taste oz accustomed high levels graphic violence EOSTOKEN SOSTOKEN not violence injustice crooked guards who'll sold nickel inmates who'll kill order away it mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz comfortable uncomfortable viewing thats touch darker side EOSTOKEN SOSTOKEN a wonderful production EOSTOKEN SOSTOKEN the filming technique unassuming old time bbc fashion comforting discomforting realism entire piece EOSTOKEN SOSTOKEN the actors extremely chosen michael sheen has polari voices pat too EOSTOKEN SOSTOKEN you seamless editing guided references williams' diary entries worth watching terrificly written performed piece EOSTOKEN SOSTOKEN a masterful production great master's comedy life EOSTOKEN SOSTOKEN the realism home things fantasy guard which traditional 'dream' techniques remains solid disappears EOSTOKEN SOSTOKEN it plays knowledge senses particularly scenes concerning orton halliwell sets particularly flat halliwell's murals decorating surface terribly done EOSTOKEN\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding tags for eos and sos\n",
    "def add_tags(corpus):\n",
    "    tagged_corpus_list = []\n",
    "    for sentence in sent_tokenize(corpus):\n",
    "        tagged_corpus_list.append('SOSTOKEN')\n",
    "        for word in sentence.split():\n",
    "            tagged_corpus_list.append(word.lower())\n",
    "        tagged_corpus_list.append('EOSTOKEN')\n",
    "\n",
    "    tagged_corpus = \" \".join(tagged_corpus_list)\n",
    "    tagged_corpus = re.sub(r\"[^A-Za-z0-9<>/s']\",\" \",tagged_corpus)\n",
    "    tagged_corpus = re.sub(r\"\\s+\",\" \",tagged_corpus)\n",
    "\n",
    "    return tagged_corpus\n",
    "\n",
    "tagged_corpus = add_tags(corpus)\n",
    "tagged_corpus_list = tagged_corpus.split()\n",
    "tagged_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_tokenizer = word_tokenize(tagged_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate bigrams and unigram freq table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram_data = {}\n",
    "# bigram_data = {}\n",
    "\n",
    "# for ind in word_tokenize(tagged_corpus):\n",
    "#     unigram_data[ind] = 0\n",
    "#     # bigram_data[ind] = {}\n",
    "#     for col in word_tokenize(tagged_corpus):\n",
    "#         bigram_data[ind][col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_freq_table(tagged_corpus,tokenizer,addOne):\n",
    "    default_freq = 0\n",
    "    if(addOne):\n",
    "        default_freq += 1\n",
    "\n",
    "    unigram_data = {}\n",
    "    bigram_data = {}\n",
    "    tagged_corpus_list = tagged_corpus.split()\n",
    "\n",
    "    for ind in tokenizer:\n",
    "        unigram_data[ind] = default_freq\n",
    "        bigram_data[ind] = {}\n",
    "        for col in tokenizer:\n",
    "            bigram_data[ind][col] = default_freq\n",
    "\n",
    "    for i in range(0,len(tagged_corpus_list)):\n",
    "        if(tagged_corpus_list[i] not in tokenizer or tagged_corpus_list[i-1] not in tokenizer):\n",
    "            continue\n",
    "        unigram_data[tagged_corpus_list[i]] += 1\n",
    "        bigram_data[tagged_corpus_list[i]][tagged_corpus_list[i-1]] += 1\n",
    "\n",
    "    return pd.Series(unigram_data),pd.DataFrame(bigram_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_table, bigram_table = get_bigram_freq_table(tagged_corpus, global_tokenizer,addOne=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate bi-gram probability table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_probability_table(tagged_corpus,tokenizer,addOne):\n",
    "    unigram_table,bigram_freq_table = get_bigram_freq_table(tagged_corpus,tokenizer,addOne)\n",
    "\n",
    "    bigram_prob_table = bigram_freq_table.copy(deep=True)\n",
    "    for col in bigram_prob_table.columns:\n",
    "        bigram_prob_table[col] = bigram_prob_table[col]/unigram_table[col]\n",
    "\n",
    "    return unigram_table/unigram_table.sum() ,bigram_prob_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_of_bigram(prev_word,curr_word,tagged_corpus,tokenizer):\n",
    "    prob_table = get_bigram_probability_table(tagged_corpus,tokenizer)\n",
    "    return prob_table[curr_word][prev_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input text to calculate probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(text,training_text,addOne):\n",
    "\n",
    "    text = add_tags(remove_stopwords_function(text))\n",
    "    training_text = add_tags(remove_stopwords_function(training_text))\n",
    "    tokenizer = word_tokenize(training_text)\n",
    "    _, bigram_prob = get_bigram_probability_table(training_text,tokenizer,addOne)\n",
    "    text_list = text.split()\n",
    "    probability = 1\n",
    "    # print(training_text)\n",
    "    i=0\n",
    "    while i < len(text_list):\n",
    "        # print(text_list[i] + ' '+str(probability))\n",
    "        if(text_list[i] not in tokenizer or text_list[i-1] not in tokenizer or bigram_prob[text_list[i]][text_list[i-1]] <= 0):\n",
    "            # print(\"Not in tokenizer\")\n",
    "            probability = probability\n",
    "        else:\n",
    "            probability = probability * bigram_prob[text_list[i]][text_list[i-1]]\n",
    "            # print(\"Found probability!\")\n",
    "        i+=1\n",
    "    return probability,bigram_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_text = input(\"Enter sentence to calculate probability: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob,bigram_probs = get_probability(\"I want an ice-cream so bad. I cant even explain how much i love ice cream.\",allText['review'][0] + ' ' + allText['review'][1] + ' '  + allText['review'][2],addOne=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of sentence is 0.015625 on basis of given text corpus\n"
     ]
    }
   ],
   "source": [
    "print(\"Probability of sentence is {} on basis of given text corpus\".format(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying add one smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob,bigram_probs = get_probability(\"I want an ice-cream so bad. I cant even explain how much i love ice cream.\",allText['review'][0] + ' ' + allText['review'][1] + ' '  + allText['review'][2],addOne=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of sentence is 0.04938271604938271 on basis of given text corpus and after add-one smoothing\n"
     ]
    }
   ],
   "source": [
    "print(\"Probability of sentence is {} on basis of given text corpus and after add-one smoothing\".format(prob))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
