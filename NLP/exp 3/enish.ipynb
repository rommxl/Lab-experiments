{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'paradigm', 'class', 'refers', 'to', 'a', 'group', 'of', 'lexemes', 'or', 'words', 'that', 'share', 'similar', 'inflectional', 'patterns', 'and', 'belong', 'to', 'the', 'same', 'grammatical', 'category', '.', 'lexemes', 'within', 'the', 'same', 'paradigm', 'class', 'exhibit', 'similar', 'variations', 'based', 'on', 'grammatical', 'features', 'such', 'as', 'tense', ',', 'number', ',', 'case', ',', 'person', ',', 'and', 'so', 'on', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def process_text_corpus(text_corpus):\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text_corpus.lower())\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    # filtered_words = [word for word in words if word not in stop_words and word not in punctuation]\n",
    "    print(words)\n",
    "# A paradigm class refers to a group of lexemes or words that share similar inflectional patterns and belong to the same grammatical category. Lexemes within the same paradigm class exhibit similar variations based on grammatical features such as tense, number, case, person, and so on.\n",
    "sample_text = input(\"Enter a text: \")\n",
    "table_data = process_text_corpus(sample_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_bigrams_with_eos(tokens):\n",
    "    \n",
    "    # Add 'eos' at the beginning and end of the list\n",
    "    tokens = ['eos'] + tokens\n",
    "    \n",
    "    # Replace end-of-sentence punctuation with 'eos'\n",
    "    tokens = [re.sub(r'[.!?]$', 'eos', token) for token in tokens]\n",
    "    \n",
    "    # Remove remaining punctuation\n",
    "    tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
    "    \n",
    "    bigram_list = []\n",
    "    for i in range(len(tokens) - 1):\n",
    "        bigram_list.append((tokens[i], tokens[i + 1]))\n",
    "    return bigram_list\n",
    "\n",
    "# Sample text data\n",
    "text = \"This is a sample text. You can replace it with your own text. What do you think? It's great!\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# Generate bigrams with 'eos' using the custom function\n",
    "bi_grams = generate_bigrams_with_eos(words)\n",
    "\n",
    "# Calculate frequency distribution for bigrams\n",
    "freq_dist = FreqDist(bi_grams)\n",
    "\n",
    "# Calculate conditional frequency distribution for bigrams\n",
    "cond_freq_dist = ConditionalFreqDist(bi_grams)\n",
    "\n",
    "# Create a probability table for bigrams\n",
    "prob_table = {}\n",
    "for bigram in freq_dist:\n",
    "    # Calculate the probability of each bigram\n",
    "    prob = freq_dist[bigram] / cond_freq_dist[bigram[0]].N()\n",
    "    prob_table[bigram] = prob\n",
    "\n",
    "# Print the probability table\n",
    "for bigram, prob in prob_table.items():\n",
    "    print(f\"{bigram}: {prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unigrams Frequency:\n",
      "+-----------+-------------+\n",
      "| Unigram   |   Frequency |\n",
      "+===========+=============+\n",
      "| eos       |           4 |\n",
      "+-----------+-------------+\n",
      "| you       |           2 |\n",
      "+-----------+-------------+\n",
      "| book      |           2 |\n",
      "+-----------+-------------+\n",
      "| a         |           2 |\n",
      "+-----------+-------------+\n",
      "| flight    |           1 |\n",
      "+-----------+-------------+\n",
      "| i         |           1 |\n",
      "+-----------+-------------+\n",
      "| read      |           2 |\n",
      "+-----------+-------------+\n",
      "\n",
      "Bigram Frequency:\n",
      "+-------------------+-------------+\n",
      "| Bigram            |   Frequency |\n",
      "+===================+=============+\n",
      "| ('eos', 'you')    |           2 |\n",
      "+-------------------+-------------+\n",
      "| ('you', 'book')   |           1 |\n",
      "+-------------------+-------------+\n",
      "| ('book', 'a')     |           1 |\n",
      "+-------------------+-------------+\n",
      "| ('a', 'flight')   |           1 |\n",
      "+-------------------+-------------+\n",
      "| ('flight', 'eos') |           1 |\n",
      "+-------------------+-------------+\n",
      "| ('eos', 'i')      |           1 |\n",
      "+-------------------+-------------+\n",
      "| ('i', 'read')     |           1 |\n",
      "+-------------------+-------------+\n",
      "| ('read', 'a')     |           1 |\n",
      "+-------------------+-------------+\n",
      "| ('a', 'book')     |           1 |\n",
      "+-------------------+-------------+\n",
      "| ('book', 'eos')   |           1 |\n",
      "+-------------------+-------------+\n",
      "| ('you', 'read')   |           1 |\n",
      "+-------------------+-------------+\n",
      "| ('read', 'eos')   |           1 |\n",
      "+-------------------+-------------+\n",
      "\n",
      "Bigram Probability Matrix:\n",
      "+--------+--------+--------+--------+----------+-----+--------+-------+\n",
      "|        |      a |   book |    eos |   flight |   i |   read |   you |\n",
      "+========+========+========+========+==========+=====+========+=======+\n",
      "| a      | 0      | 0.2222 | 0      |   0.2222 | 0   | 0      |   0   |\n",
      "+--------+--------+--------+--------+----------+-----+--------+-------+\n",
      "| book   | 0.2222 | 0      | 0.2222 |   0      | 0   | 0      |   0   |\n",
      "+--------+--------+--------+--------+----------+-----+--------+-------+\n",
      "| eos    | 0      | 0      | 0      |   0      | 0.2 | 0      |   0.3 |\n",
      "+--------+--------+--------+--------+----------+-----+--------+-------+\n",
      "| flight | 0      | 0      | 0.25   |   0      | 0   | 0      |   0   |\n",
      "+--------+--------+--------+--------+----------+-----+--------+-------+\n",
      "| i      | 0      | 0      | 0      |   0      | 0   | 0.25   |   0   |\n",
      "+--------+--------+--------+--------+----------+-----+--------+-------+\n",
      "| read   | 0.2222 | 0      | 0.2222 |   0      | 0   | 0      |   0   |\n",
      "+--------+--------+--------+--------+----------+-----+--------+-------+\n",
      "| you    | 0      | 0.2222 | 0      |   0      | 0   | 0.2222 |   0   |\n",
      "+--------+--------+--------+--------+----------+-----+--------+-------+\n",
      "\n",
      "Bigrams in input sentence:\n",
      "[('eos', 'you'), ('you', 'read'), ('read', 'a'), ('a', 'book'), ('book', 'eos')]\n",
      "\n",
      "Probability of given sentence: 0.000732\n",
      "\n",
      "After Add-One Smoothing:\n",
      "\n",
      "Bigram Probability Matrix:\n",
      "+--------+-----+--------+--------+----------+--------+--------+--------+\n",
      "|        |   a |   book |    eos |   flight |      i |   read |    you |\n",
      "+========+=====+========+========+==========+========+========+========+\n",
      "| a      | 0   |    0.2 | 0      |      0.2 | 0      | 0      | 0      |\n",
      "+--------+-----+--------+--------+----------+--------+--------+--------+\n",
      "| book   | 0.2 |    0   | 0.2    |      0   | 0      | 0      | 0      |\n",
      "+--------+-----+--------+--------+----------+--------+--------+--------+\n",
      "| eos    | 0   |    0   | 0      |      0   | 0.1818 | 0      | 0.2727 |\n",
      "+--------+-----+--------+--------+----------+--------+--------+--------+\n",
      "| flight | 0   |    0   | 0.2222 |      0   | 0      | 0      | 0      |\n",
      "+--------+-----+--------+--------+----------+--------+--------+--------+\n",
      "| i      | 0   |    0   | 0      |      0   | 0      | 0.2222 | 0      |\n",
      "+--------+-----+--------+--------+----------+--------+--------+--------+\n",
      "| read   | 0.2 |    0   | 0.2    |      0   | 0      | 0      | 0      |\n",
      "+--------+-----+--------+--------+----------+--------+--------+--------+\n",
      "| you    | 0   |    0.2 | 0      |      0   | 0      | 0.2    | 0      |\n",
      "+--------+-----+--------+--------+----------+--------+--------+--------+\n",
      "\n",
      "Probability of given sentence (After Add-One Smoothing): 0.000436\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "\n",
    "def preprocess(text):\n",
    "    text = nltk.word_tokenize(text.lower())\n",
    "    text = ['eos'] + text\n",
    "    text = [re.sub(r'[.!?]$', 'eos', token) for token in text]\n",
    "    text = [re.sub(r'[^\\w\\s]', '', token) for token in text]\n",
    "    return text\n",
    "\n",
    "def create_bigrams(data):\n",
    "    bigrams = []\n",
    "    bigram_counts = {}\n",
    "    unigram_counts = {}\n",
    "    for i in range(len(data) - 1):\n",
    "        if i < len(data) - 1 and data[i + 1].islower():\n",
    "            bigrams.append((data[i], data[i + 1]))\n",
    "\n",
    "            if (data[i], data[i + 1]) in bigram_counts:\n",
    "                bigram_counts[(data[i], data[i + 1])] += 1\n",
    "            else:\n",
    "                bigram_counts[(data[i], data[i + 1])] = 1\n",
    "\n",
    "        if data[i] in unigram_counts:\n",
    "            unigram_counts[data[i]] += 1\n",
    "        else:\n",
    "            unigram_counts[data[i]] = 1\n",
    "    return bigrams, unigram_counts, bigram_counts\n",
    "\n",
    "def calc_bigram_prob(bigrams, unigram_counts, bigram_counts, vocab_size):\n",
    "    prob_dict = {}\n",
    "    for bigram in bigrams:\n",
    "        word1, word2 = bigram\n",
    "        prob_dict[bigram] = (bigram_counts.get(bigram, 0) + 1) / (unigram_counts.get(word1, 0) + vocab_size)\n",
    "    return prob_dict\n",
    "\n",
    "def gen_bigrams(tokens):\n",
    "    return [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "\n",
    "def calc_unigram_freq(tokens):\n",
    "    unigram_freq = defaultdict(int)\n",
    "    for word in tokens:\n",
    "        unigram_freq[word] += 1\n",
    "    return unigram_freq\n",
    "\n",
    "def calc_bigram_freq(bigrams):\n",
    "    bigram_freq = defaultdict(int)\n",
    "    for bigram in bigrams:\n",
    "        bigram_freq[bigram] += 1\n",
    "    return bigram_freq\n",
    "\n",
    "def calc_bigram_probs(bigram_freq, unigram_freq, vocab_size):\n",
    "    bigram_probs = defaultdict(float)\n",
    "    for bigram in bigram_freq:\n",
    "        bigram_probs[bigram] = bigram_freq[bigram] / unigram_freq[bigram[0]]\n",
    "    return bigram_probs\n",
    "\n",
    "def bi_matrix(bigrams, bigram_probs):\n",
    "    unique_words = list(set([word for bigram in bigrams for word in bigram]))\n",
    "    unique_words.sort()\n",
    "\n",
    "    matrix_data = []\n",
    "    for word1 in unique_words:\n",
    "        row = [word1] + [f\"{bigram_probs.get((word1, word2), 0):.4f}\" for word2 in unique_words]\n",
    "        matrix_data.append(row)\n",
    "\n",
    "    headers = [''] + unique_words\n",
    "    table = tabulate(matrix_data, headers, tablefmt=\"grid\")\n",
    "    print(\"\\nBigram Probability Matrix:\")\n",
    "    print(table)\n",
    "\n",
    "def display_bi_freq(bigram_freq):\n",
    "    table_data = [(bigram, freq) for bigram, freq in bigram_freq.items()]\n",
    "    print(\"\\nBigram Frequency:\")\n",
    "    print(tabulate(table_data, headers=[\"Bigram\", \"Frequency\"], tablefmt=\"grid\"))\n",
    "\n",
    "def display_uni_freq(unigram_freq):\n",
    "    table_data = [(unigram, freq) for unigram, freq in unigram_freq.items()]\n",
    "    print(\"\\nUnigrams Frequency:\")\n",
    "    print(tabulate(table_data, headers=[\"Unigram\", \"Frequency\"], tablefmt=\"grid\"))\n",
    "\n",
    "def bigram_list_ip(sentence_bigrams):\n",
    "    print(\"\\nBigrams in input sentence:\")\n",
    "    print(sentence_bigrams)\n",
    "\n",
    "def calc_sentence_prob(sentence, bigram_probs):\n",
    "    sentence_bigrams = gen_bigrams(sentence)\n",
    "    prob = 1.0\n",
    "    for bigram in sentence_bigrams:\n",
    "        prob *= bigram_probs.get(bigram, 0)\n",
    "    return prob\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    corpus = input(\"Enter training corpus\")\n",
    "    # corpus = \"(eos) You book a flight (eos) I read a book (eos) You read (eos)\"\n",
    "    preproc_corpus = preprocess(corpus)\n",
    "    tokens = preproc_corpus\n",
    "\n",
    "    bigrams, unigram_counts, bigram_counts = create_bigrams(tokens)\n",
    "\n",
    "    unigram_freq = calc_unigram_freq(tokens)\n",
    "    bigram_freq = calc_bigram_freq(bigrams)\n",
    "\n",
    "    vocab_size = len(set(tokens))\n",
    "    bigram_probs = calc_bigram_prob(bigrams, unigram_counts, bigram_counts, vocab_size)\n",
    "\n",
    "    display_uni_freq(unigram_freq)\n",
    "    display_bi_freq(bigram_freq)\n",
    "\n",
    "    bi_matrix(bigrams, bigram_probs)\n",
    "\n",
    "    ip_sentence = input(\"Enter test sentence\")\n",
    "    # ip_sentence = \"(eos) You read a book (eos)\"\n",
    "    ip_tokens = preprocess(ip_sentence)\n",
    "\n",
    "    bigram_list_ip(gen_bigrams(ip_tokens))\n",
    "\n",
    "    ip_prob = calc_sentence_prob(ip_tokens, bigram_probs)\n",
    "    print(f\"\\nProbability of given sentence: {ip_prob:.6f}\")\n",
    "\n",
    "    bigram_probs_smoothed = calc_bigram_prob(bigrams, unigram_counts, bigram_counts, vocab_size + 1)\n",
    "\n",
    "    print(\"\\nAfter Add-One Smoothing:\")\n",
    "    bi_matrix(bigrams, bigram_probs_smoothed)\n",
    "\n",
    "    ip_prob_smoothed = calc_sentence_prob(ip_tokens, bigram_probs_smoothed)\n",
    "    print(f\"\\nProbability of given sentence (After Add-One Smoothing): {ip_prob_smoothed:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
